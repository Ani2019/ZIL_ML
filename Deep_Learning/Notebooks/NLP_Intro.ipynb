{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing):\n",
    "> Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "\n",
    "## Some appliacations:\n",
    "\n",
    "[Check this](https://towardsdatascience.com/natural-language-processing-nlp-top-10-applications-to-know-b2c80bd428cb)\n",
    "\n",
    "* Machine Translation\n",
    "* Speech Recognition\n",
    "* Sentiment Analysis\n",
    "* Chatbots\n",
    "* Automatic Summarization\n",
    "* Text Classification\n",
    "* etc.\n",
    "\n",
    "## Some tasks:\n",
    "\n",
    "[Check this](https://en.wikipedia.org/wiki/Natural_language_processing#Major_evaluations_and_tasks)\n",
    "\n",
    "* POS tagging\n",
    "* Parsing\n",
    "* NER\n",
    "* NLU\n",
    "* NLG\n",
    "* etc.\n",
    "\n",
    "## History of NLP\n",
    "\n",
    "[Check this](https://ruder.io/a-review-of-the-recent-history-of-nlp/index.html#2013wordembeddings) or [this](https://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/)\n",
    "![](../Images/neural_history_of_nlp_image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "\n",
    "* A chapter from \"Speech and Language Processing\" book by  Dan Jurafsky and James H. Martin - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "## Word2Vec and Glove\n",
    "\n",
    "* word2vec paper here - https://arxiv.org/pdf/1301.3781.pdf\n",
    "\n",
    "* Slides from prof. C.Manning - http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture01-wordvecs1.pdf\n",
    "\n",
    "* Tutorial - http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "* A nice post for mathematical derivations - https://medium.com/analytics-vidhya/maths-behind-word2vec-explained-38d74f32726b\n",
    "\n",
    "* word2vec algorithm improvements - http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ (negative sampling and subsampling frequent words\n",
    "\n",
    "* Richard Socher's lecture for word2vec improvements and Glove - https://www.youtube.com/watch?v=ASn7ExxLZws\n",
    "\n",
    "* Richard Socher's corresponding slides - https://cs224d.stanford.edu/lectures/CS224d-Lecture3.pdf\n",
    "\n",
    "* word2vec implementation with python - https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281\n",
    "\n",
    "* Glove paper here - https://www-nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "* Glove tutorial here - https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010\n",
    "\n",
    "* A nice explanation of math behind the Glove algorithm - https://www.youtube.com/watch?v=Fn_U2OG1uqI&t=1068s\n",
    "\n",
    "* Glove implementation - http://www.foldl.me/2014/glove-python/\n",
    "\n",
    "* Word embeddings with tensorflow - https://www.tensorflow.org/tutorials/text/word_embeddings)\n",
    "\n",
    "* Glove with Keras - https://github.com/thushv89/exercises_thushv_dot_com/blob/master/glove_light_on_math_ml/glove_light_on_math_ml.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature guide\n",
    "\n",
    "[Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/index.html#schedule) Stanford course taught by Christopher Manning\n",
    "\n",
    "Corresponding [video course here](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)\n",
    "\n",
    "[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) a book by Dan Jurafsky and James H. Martin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries \n",
    "\n",
    "* [Gensim](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
    "* [NLTK](https://www.nltk.org/)\n",
    "* [Spacy](https://spacy.io/)\n",
    "\n",
    "## Applications\n",
    "\n",
    "* [word2vec on Simpson's conversations data](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial)\n",
    "* [word2vec on vehicle dataset](https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

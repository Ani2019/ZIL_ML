{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main sources used\n",
    "> * Deep Learning a book by Ian Goodfellow at. al. http://www.deeplearningbook.org/contents/optimization.html\n",
    "> * Stanford course taught by Andrej Karpathy http://cs231n.stanford.edu/slides/2016/winter1516_lecture6.pdf\n",
    "> * An overview of gradient descent optimizationalgorithms by Sebastian Ruder https://arxiv.org/pdf/1609.04747.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Challenges and Problems\n",
    "\n",
    "* Ill Conditioning\n",
    "\n",
    "when a small change in the independent variable (input) leads to a large change in the dependent variable (output).\n",
    "* Local Minima\n",
    "\n",
    "non-convex functions such as nn's have many local minimas\n",
    "* Saddle Points and Flat Regions\n",
    "\n",
    "in high-dimensional non-convex functions this type of points occur more\n",
    "* Cliffs\n",
    "* Vanishing or Exploding Gradients\n",
    "\n",
    "Vanising gradients are mainly a result of long-term dependencies and expldoing gradients are occuring when there are cliffs \n",
    "* Inexact Gradients\n",
    "\n",
    "mini-batches bring noise\n",
    "* Choosing the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's discuss the types of gradient descent algorithms depending on batch size. Check [this](https://arxiv.org/pdf/1609.04747.pdf) paper.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "This is vanilla gradient descent algorithm (batch gradient descent).\n",
    "The gradients (partial derivatives of loss functions w.r.t.model paramters) are calculated for the entire training dataset:\n",
    "$$\n",
    "\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J(\\theta)\n",
    "$$\n",
    "~~~python\n",
    "for i in range(nb_epochs):\n",
    "    params_grad = evaluate_gradient (loss_function, data, params)\n",
    "    params = params - learning_rate * params_grad\n",
    "~~~\n",
    "\n",
    "Batch gradient descent is **SLOW**.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent performs a parameter update for each training example\n",
    "$x^{(i)}$ and label $y^{(i)}:$\n",
    "$$\n",
    "\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i)} ; y^{(i)}\\right)\n",
    "$$\n",
    "\n",
    "~~~python\n",
    "for i in range(nb_epochs ):\n",
    "    np.random.shuffle(data)\n",
    "    for  example  in data:\n",
    "        params_grad = evaluate_gradient(loss_function , example , params)\n",
    "        params = params  - learning_rate * params_grad\n",
    "~~~\n",
    "\n",
    "Updates are very stochastic and lead to **fluctuation**.\n",
    "\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "\n",
    "Mini-batch gradient descent takes the best of both and performs an update for every mini-batch of $n$ training examples:\n",
    "$$\n",
    "\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i: i+n)} ; y^{(i: i+n)}\\right)\n",
    "$$\n",
    "\n",
    "~~~python\n",
    "for i in range(nb_epochs ):\n",
    "    np.random.shuffle(data)\n",
    "    for  batch  in  get_batches(data , batch_size =50):\n",
    "        params_grad = evaluate_gradient(loss_function , batch , params)\n",
    "        params = params  - learning_rate * params_grad\n",
    "~~~\n",
    "\n",
    "Why are mini-batches a good choice?\n",
    "* Not very stochastic, gives some **stability*(\n",
    "* **Optimal** computation\n",
    "    \n",
    "An interesting motivation for minibatch stochastic gradient descent is that it follows the gradient of the true generalization error  $J^{*}(\\boldsymbol{\\theta})=\\mathbb{E}_{(\\boldsymbol{x}, y) \\sim p_{\\text {data }}} L(f(\\boldsymbol{x} ; \\boldsymbol{\\theta}), y)$ as long as no examples are repeated. (Check 8.1.3 paragraph [here](http://www.deeplearningbook.org/contents/optimization.html)). So it is possible to obtain an unbiased estimate of the gradient by taking the average gradient on a minibatch of m examples drawn i.i.d from the data-generating distribution.\n",
    "    \n",
    "Some mini-batch facts!\n",
    "* Parallel usage of batch samples causes larger batch = larger memory scaling\n",
    "* Specific batch sizes are recommended to be used depending on the hardware\n",
    "* Best generaliation error occurs at batch size = 1\n",
    "* Mini batch size should be logically proportional to the number of classes\n",
    "* Mini batches must be selected randomly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html) are some nice animations for different optimization methods by Alec Radford.\n",
    "![opt2](../Images/opt2.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "Nowadays we use SGD term meaning mini-batch stochastic gradient descent. It is the most common choice from the above options.\n",
    "\n",
    "It is a common practice to reduce learning rate during training process. Let's look on such algorithm of SGD. \n",
    "\n",
    "We denote the learning rate at iteration $k$ as $\\epsilon_k$. \n",
    "In practice, it is common to decay the learning rate linearly until iteration $\\tau$:\n",
    "$$\n",
    "\\epsilon_{k}=(1-\\alpha) \\epsilon_{0}+\\alpha \\epsilon_{\\tau}\n",
    "$$\n",
    "with $\\alpha=\\frac{k}{\\tau} .$ After iteration $\\tau,$ it is common to leave $\\epsilon$ constant.\n",
    "\n",
    "![sgd](../Images/SGD.png)\n",
    "\n",
    "The main challenge here is the selection of learning rate.\n",
    "\n",
    "## SGD with Momentum\n",
    "\n",
    "Formally, the momentum algorithm introduces a variable $\\boldsymbol{v}$ that plays the role of velocity- it is the direction and speed at which the parameters move through parameter space. The velocity is set to an exponentially decaying average of the negative gradient. The name momentum derives from a [physical analogy](https://en.wikipedia.org/wiki/Momentum).\n",
    "\n",
    "This allows velocity to build up in shallow directions and be damped in sttep ones.\n",
    "\n",
    "![momentum](../Images/momentum.png)\n",
    "\n",
    "Physical interpretation: imagine a ball rolling down the loss function with friction ($\\alpha$ coefficient). $\\alpha=\\text { usually } \\sim 0.5,0.9, \\text { or } 0.99 \\text { (Sometimes annealed over time, e.g. from 0.5 } \\rightarrow 0.99)$\n",
    "\n",
    "\n",
    "## Nesterov Momentum\n",
    "\n",
    "This is a variant of Momentum wich usually works better.\n",
    "\n",
    "![netserov](../Images/nesterov.png)\n",
    "\n",
    "![mom_vs_nest](../Images/mom_vs_nest.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "\n",
    "AdaGrad individually adaptes learning rates of model parameters.\n",
    "Large partial derivatives lead to more rapid learning rate decays.\n",
    "\n",
    "For deep neural networks AdaGrad can cause premature termination of learning process as the learning rate will become very small quite soon.\n",
    "\n",
    "![adagrad](../Images/adagrad.png)\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "RMSProp comes to improve AdaGrad. It changes the gradient accumulation into an exponentially weighted moving average to avoid keeping track of \"ancient history\".\n",
    "\n",
    "![rmsprop](../Images/rmsprop.png)\n",
    "\n",
    "\n",
    "## Adam\n",
    "\n",
    "$Momentum + RMSProp \\approx Adam$\n",
    "\n",
    "This is one of the most used optimizers so far. [This](https://arxiv.org/pdf/1412.6980.pdf) is the original paper.\n",
    "\n",
    "Not only combining RMSProp and Momentum, Adam also implements [bias correction](https://www.youtube.com/watch?v=Zs4qJN-I5Kk). Default settings mentioned in the paper are quite robust and you won't need to change them in most of the cases.\n",
    "\n",
    "![adam](../Images/adam_paper.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

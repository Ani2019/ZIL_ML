{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative vs Discriminative\n",
    "\n",
    "_https://developers.google.com/machine-learning/gan/generative_\n",
    "\n",
    "* **Generative** models can generate new data instances.\n",
    "* **Discriminative** models discriminate between different kinds of data instances.\n",
    "\n",
    "More formally, given a set of data instances X and a set of labels Y:\n",
    "\n",
    "* **Generative** models capture the joint probability p(X, Y), or just p(X) if there are no labels.\n",
    "* **Discriminative** models capture the conditional probability p(Y | X).\n",
    "\n",
    "![gen_vs_disc](../Images/generative_vs_discriminative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation learning\n",
    "\n",
    "* http://www.deeplearningbook.org/contents/representation.html\n",
    "\n",
    "* https://arxiv.org/pdf/1206.5538.pdf\n",
    "\n",
    "Feature learning or learning the representations.\n",
    "\n",
    "Unsupervised pre-training for neural nets http://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf (though quite old idea already)\n",
    "    \n",
    "Advantages of Unsupervised or self-supervised learning with lack of data\n",
    "\n",
    "\n",
    "# Restricted Boltzmann Machines (RBM)\n",
    "\n",
    "theory\n",
    "* https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf (original paper)\n",
    "* https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15\n",
    "* https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf\n",
    "* http://www.dmi.usherb.ca/~larocheh/neural_networks/content.html (video course and slides)\n",
    "* https://www.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf\n",
    "* https://www.cs.toronto.edu/~hinton/csc2535/notes/lec4new.pdf\n",
    "* https://pathmind.com/wiki/restricted-boltzmann-machine\n",
    "\n",
    "implementation:\n",
    "* https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-ii-4b159dce1ffb\n",
    "\n",
    "\n",
    "# Autoencoders\n",
    "\n",
    "* https://www.deeplearningbook.org/contents/autoencoders.html\n",
    "* http://www.dmi.usherb.ca/~larocheh/neural_networks/content.html\n",
    "* http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf\n",
    "* https://medium.com/edureka/autoencoders-tutorial-cfdcebdefe37\n",
    "* https://www.jeremyjordan.me/autoencoders/\n",
    "\n",
    "![autoencoder](../Images/autoencoder.png)\n",
    "\n",
    "![autoenc](../Images/autoenc.png)\n",
    "\n",
    "Autoencoder model consists of two sub-models: encoder model and decoder model.\n",
    "\n",
    "**L2 loss or Cross-entropy loss is used after the decoder as reconstruction loss**\n",
    "\n",
    "Depending on the number of hidden units we can distinguish between undercomplete or overcomplete autoencoders.\n",
    "\n",
    "Autoencoders are data-specific!\n",
    "They are lossy!\n",
    "Though they are learned atomatically :) \n",
    "\n",
    "The use cases:\n",
    "\n",
    "* We can use a traditional supervised algorithm on top of an trained encoder model.  \n",
    "* Dimensionality reduction (for data visualization mainly)\n",
    "* Data compression\n",
    "* Image denoising\n",
    "* Image coloring\n",
    "\n",
    "Originally autoencoders used to have Linear + nonlinearity(sigmoid) form. Nowadays autoencoders use deep, fuly-connected layers or convolutions. These type of autoencoders have more layers and thus are called deep autoencoders. \n",
    "\n",
    "There are many types of autoencoders nowadays: Some of the famous ones are:\n",
    "* Convolution Autoencoders\n",
    "* Deep Autoencoders\n",
    "* Sparse Autoencoders\n",
    "* Denoising Autoencoders\n",
    "* Variational Autoencoders (VAE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE (Variational Autoencoder)\n",
    "\n",
    "* https://arxiv.org/pdf/1312.6114.pdf (original paper)\n",
    "* https://arxiv.org/pdf/1906.02691.pdf (comprehensive guide from authors)\n",
    "* https://www.youtube.com/watch?v=5WoItGTWV54&t=3366s (stanford lecture)\n",
    "\n",
    "\n",
    "![vae](../Images/vae.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

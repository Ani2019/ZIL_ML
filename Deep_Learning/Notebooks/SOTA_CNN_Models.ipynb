{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../Images/deeper.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet 1998\n",
    "\n",
    "##### original paper: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n",
    "\n",
    "![image.png](../Images/lenet.png)\n",
    "\n",
    "1. Conv Layer -> filters = 6, kernel_size = (5,5), strides = (1,1),activation=\"tanh\"\n",
    "2. Max Pool Layer -> (2,2) filter size and (2,2 stride)\n",
    "3. Conv Layer -> filters = 16, kernel_size = (5,5), strides = (1,1),activation=\"tanh\"\n",
    "4. Max Pool Layer -> (2,2) filter size and (2,2 stride)\n",
    "5. FC -> 120 \n",
    "6. FC -> 84 \n",
    "7. FC -> 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet  (2012)\n",
    "###### modified LeNet\n",
    "###### original paper:https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Main Points\n",
    "\n",
    "###### Trained the network on ImageNet data, which contained over 15 million annotated images from a total of over 22,000 categories.\n",
    "###### Used ReLU for the nonlinearity functions (Found to decrease training time as ReLUs are several times faster than the conventional tanh function).\n",
    "###### Used data augmentation techniques that consisted of image translations, horizontal reflections, and patch extractions.\n",
    "###### Implemented dropout layers in order to combat the problem of overfitting to the training data.\n",
    "###### Trained the model using batch stochastic gradient descent, with specific values for momentum and weight decay.\n",
    "###### Trained on two GTX 580 GPUs for five to six days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../Images/alexnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Net (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### original paper:https://arxiv.org/pdf/1409.1556.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../Images/vgg16_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../Images/vgg16_p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../Images/vgg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Main Points\n",
    "\n",
    "###### The use of only 3x3 sized filters is quite different from AlexNet’s 11x11 filters in the first layer and ZF Net’s 7x7 filters. The authors’ reasoning is that the combination of two 3x3 conv layers has an effective receptive field of 5x5. This in turn simulates a larger filter while keeping the benefits of smaller filter sizes. One of the benefits is a decrease in the number of parameters. Also, with two conv layers, we’re able to use two ReLU layers instead of one.\n",
    "###### 3 conv layers back to back have an effective receptive field of 7x7.\n",
    "###### As the spatial size of the input volumes at each layer decrease (result of the conv and pool layers), the depth of the volumes increase due to the increased number of filters as you go down the network.\n",
    "###### Interesting to notice that the number of filters doubles after each maxpool layer. This reinforces the idea of shrinking spatial dimensions, but growing depth.\n",
    "###### Worked well on both image classification and localization tasks. The authors used a form of localization as regression (see page 10 of the paper for all details).\n",
    "###### Built model with the Caffe toolbox.\n",
    "###### Used scale jittering as one data augmentation technique during training.\n",
    "###### Used ReLU layers after each conv layer and trained with batch gradient descent.\n",
    "###### Trained on 4 Nvidia Titan Black GPUs for two to three weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet or InceptionNet(2015)\n",
    "##### original paper: https://arxiv.org/pdf/1409.4842.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../Images/inception.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../Images/inc_module.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Main Points\n",
    "\n",
    "###### Used 9 Inception modules in the whole architecture, with over 100 layers in total! Now that is deep…\n",
    "###### No use of fully connected layers! They use an average pool instead, to go from a 7x7x1024 volume to a 1x1x1024 volume. This saves a huge number of parameters.\n",
    "###### Uses 12x fewer parameters than AlexNet.\n",
    "###### During testing, multiple crops of the same image were created, fed into the network, and the softmax probabilities were averaged to give us the final solution.\n",
    "###### Utilized concepts from R-CNN (a paper we’ll discuss later) for their detection model.\n",
    "###### There are updated versions to the Inception module (Versions 6 and 7).\n",
    "###### Trained on “a few high-end GPUs within a week”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft ResNet (2015)\n",
    "##### original paper: https://arxiv.org/pdf/1512.03385.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../Images/resnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../Images/resid_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### “Ultra-deep” – Yann LeCun.\n",
    "###### 152 layers…\n",
    "###### Interesting note that after only the first 2 layers, the spatial size gets compressed from an input volume of 224x224 to a 56x56 volume.\n",
    "###### Authors claim that a naïve increase of layers in plain nets result in higher training and test error (Figure 1 in the paper).\n",
    "###### The group tried a 1202-layer network, but got a lower test accuracy, presumably due to overfitting.\n",
    "###### Trained on an 8 GPU machine for two to three weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comporasion on ImageNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../Images/comporision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://paperswithcode.com/sota/image-classification-on-imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refferances \n",
    "###### https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\n",
    "###### https://www.youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

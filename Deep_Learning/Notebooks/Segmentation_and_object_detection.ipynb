{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks Taxonomy\n",
    "![](../Images/det_tasks.jpeg)\n",
    "![](../Images/inst_seg.png)\n",
    "##### Real time semantic segmentation example: https://www.youtube.com/watch?v=ATlcEDSPWXY\n",
    "##### Real time instance segmentation example: https://www.youtube.com/watch?v=0pMfmo8qfpQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets \n",
    "\n",
    "##### PASCAL Visual Object Classes (PASCAL VOC)\n",
    "\n",
    "The PASCAL VOC dataset (2012) is well-known an commonly used for object detection and segmentation. More than 11k images compose the train and validation datasets while 10k images are dedicated to the test dataset.\n",
    "\n",
    "##### Common Objects in COntext ([COCO](http://cocodataset.org/#explore))\n",
    "\n",
    "There are two COCO challenges (in 2017 and 2018) for image semantic segmentation (“object detection” and “stuff segmentation”). The “object detection” task consists in segmenting and categorizing objects into 80 categories. The “stuff segmentation” task uses data with large segmented part of the images (sky, wall, grass), they contain almost the entire visual information. In this blog post, only the results of the “object detection” task will be compared because too few of the quoted research papers have published results on the “stuff segmentation” task.\n",
    "\n",
    "##### Cityscapes\n",
    "\n",
    "The Cityscapes dataset has been released in 2016 and consists in complex segmented urban scenes from 50 cities. It is composed of 23.5k images for training and validation (fine and coarse annotations) and 1.5 images for testing (only fine annotation)\n",
    "\n",
    "# Evaluation metrics\n",
    "\n",
    "#### IoU(Intersection over union)\n",
    "\n",
    "![](../Images/IoU.png)\n",
    "\n",
    "***Average Precision*** is defined as the area under the precision-recall curve (PR curve).In the case of objection detection or instance segmentation, the multiple precision-recall value is done by changing the score cutoff.\n",
    "\n",
    " In COCO they change the IoU threshold values from 50% to 95%, at a step of 5%. So we end up with 10 precision-recall pairs. If we take the average of those 10 values, we get ***AP[0.5:0.95]***.\n",
    " \n",
    "**mAP is simply all the AP values averaged over different classes/categories**\n",
    "\n",
    "[more](https://medium.com/@yanfengliux/the-confusing-metrics-of-ap-and-map-for-object-detection-3113ba0386ef)\n",
    "\n",
    "[losses](https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Models\n",
    "\n",
    "### Fully Convolutional Networks (FCN)\n",
    "[H. Noh et al. (2015)](https://arxiv.org/pdf/1505.04366.pdf)\n",
    "\n",
    "![](../Images/segnet.png)\n",
    "\n",
    "\n",
    "\n",
    "##### Previous papers FCN [J. Long et al. (2015)](https://arxiv.org/abs/1605.06211), ParseNet [W. Liu et al. (2015)](https://arxiv.org/pdf/1506.04579.pdf)\n",
    "\n",
    "### Unpooling \n",
    "\n",
    "![](../Images/upscale.png)\n",
    "\n",
    "\"It records the locations of maximum activations selected during pooling operation in switch variables, which are employed to place each activation back to its original pooled\n",
    "location\" J. Long et al. (2015)\n",
    "![](../Images/upsample.png)\n",
    "\n",
    "\n",
    "### [Transposed convolution](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### [U-NET](https://arxiv.org/pdf/1505.04597.pdf)\n",
    "![](../Images/U-net.png)\n",
    "Note that it doesn’t use any fully-connected layer. As consequencies, the number of parameters of the model is reduced and it can be trained with a small labelled dataset (using appropriate data augmentation). For example, the authors have used a public dataset with 30 images for training during their experiments.\n",
    "![](../Images/unet_res.png)\n",
    "\n",
    "[more models](https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with localization\n",
    "\n",
    "![](../Images/class+loc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "\n",
    "## Interesting topics\n",
    "* Sliding Window\n",
    "* Image Pyramides\n",
    "* Non Max Supression\n",
    "\n",
    "All these ideas are behind  [OverFeat](https://arxiv.org/abs/1312.6229) model\n",
    "\n",
    "## Region-based Convolutional Network ([R-CNN](http://islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf))\n",
    "The first models intuitively begin with the region search and then perform the classification.\n",
    "\n",
    "\n",
    "In R-CNN, the selective search method developed by [J.R.R. Uijlings and al. (2012)](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf) is an alternative to exhaustive search in an image to capture object location. It initializes small regions in an image and merges them with a hierarchical grouping. Thus the final group is a box containing the entire image. The detected regions are merged according to a variety of color spaces and similarity metrics. The output is a few number of region proposals which could contain an object by merging small regions.\n",
    "\n",
    "[Selective search ideas](https://www.youtube.com/watch?v=uX4LLf-33p0&list=PL1GQaVhO4f_jLxOokW7CS5kY_J1t1T17S&index=60)\n",
    "\n",
    "![](../Images/select_search.png)\n",
    "Each region proposal is resized to match the input of a CNN from which we extract a 4096-dimension vector of features.\n",
    "Each one of these classes has a SVM classifier trained to infer a probability to detect this object for a given vector of features. This vector also feeds a linear regressor to adapt the shapes of the bounding box for a region proposal and thus reduce localization errors.\n",
    "![](../Images/rcnn.png)\n",
    "\n",
    "\n",
    "\n",
    "## Fast Region-based Convolutional Network ([Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf))\n",
    "A main CNN with multiple convolutional layers is taking the entire image as input instead of using a CNN for each region proposals (R-CNN). Region of Interests (RoIs) are detected with the selective search method applied on the produced feature maps. Formally, the feature maps size is reduced using a [RoI Pooling](https://towardsdatascience.com/region-of-interest-pooling-f7c637f409af) layer to get valid Region of Interests with fixed heigh and width as hyperparameters. Each RoI layer feeds fully-connected layers¹ creating a features vector. The vector is used to predict the observed object with a softmax classifier and to adapt bounding box localizations with a linear regressor.\n",
    "\n",
    "![](../Images/fast_rcnn.png)\n",
    "\n",
    "## Faster R-CNN\n",
    "Region proposals detected with the selective search method were still necessary in the previous model, which is computationally expensive. S. Ren and al. (2016) have introduced Region Proposal Network (RPN) to directly generate region proposals, predict bounding boxes and detect objects. The Faster Region-based Convolutional Network (Faster R-CNN) is a combination between the RPN and the Fast R-CNN model.\n",
    "\n",
    "A CNN model takes as input the entire image and produces feature maps. A window of size 3x3 slides all the feature maps and outputs a features vector linked to two fully-connected layers, one for box-regression and one for box-classification. Multiple region proposals are predicted by the fully-connected layers. A maximum of k regions is fixed thus the output of the box-regression layer has a size of 4k (coordinates of the boxes, their height and width) and the output of the box-classification layer a size of 2k (“objectness” scores to detect an object or not in the box). The k region proposals detected by the sliding window are called anchors.\n",
    "![](../Images/anchor_nums.png)\n",
    "[Absolute vs Relative BBOX Regression | Anchor Boxes](https://www.youtube.com/watch?v=AVTs_N8YhBw)\n",
    "\n",
    "\n",
    "![](../Images/fast-rcnn.png)\n",
    "\n",
    "![](../Images/fast_rccn2.png)\n",
    "\n",
    "## YOLO:https://pjreddie.com/darknet/yolo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Segmentation\n",
    "## [Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf)\n",
    "![](../Images/Mask-RCNN.jpg)\n",
    "#### ROI allign:https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERANCES\n",
    "\n",
    "https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57\n",
    "\n",
    "https://www.youtube.com/watch?v=9I6nzfx_kpE&list=PL1GQaVhO4f_jLxOokW7CS5kY_J1t1T17S&index=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

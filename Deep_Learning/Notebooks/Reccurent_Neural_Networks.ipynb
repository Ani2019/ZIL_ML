{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> all images are taken from [Colah's bolg](http://colah.github.io/)\n",
    "\n",
    "**To model sequences, we need to:**\n",
    "1. Handle variable-length sequences\n",
    "2. Track long-term dependencies\n",
    "3. Maintain information about order\n",
    "4. Share parameters across the sequence\n",
    "\n",
    "## RNNs\n",
    "\n",
    "Applies a recurrent relation at every time step for processing the sequence.\n",
    "\n",
    "**Important**: The same function is used at every time step and parameters are shared!\n",
    "\n",
    "![](../Images/RNN-unrolled.png)\n",
    "![](../Images/SimpleRNN.png)\n",
    "\n",
    "The loss function is computed per output at each time step. The total loss is the sum of those.\n",
    "Backporpagation is tough for simple RNN's as we have to backprop through time! This simply leads to gradient exploding/vanishing problems which themselves lead to a problem with long-term dependencies. \n",
    "\n",
    "Shortly, Vanilla RNN's are good enough for processing short sequneces but that's it.\n",
    "\n",
    "## LSTMs\n",
    "\n",
    "Long Short Term Memory Networks were designed to solve the long-term dependency problem with vanilla RNNs. They are a special type of RNN.\n",
    "\n",
    "The key to LSTMs is the cell state, the horizontal line running through the top of the diagram which eases the gradient flow during backprop.\n",
    "\n",
    "The other major difference is the presence of gates which are capable of changing the cell state and output.\n",
    "\n",
    "There are 4 gates which correspondingly perform actions:\n",
    "1. Forget\n",
    "2. Store\n",
    "3. Update\n",
    "4. Output\n",
    "\n",
    "![](../Images/LSTM.png)\n",
    "\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "**_Please follow the first two links in literature section for more_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Literature**\n",
    "\n",
    "* A sweet post about main issues by Christopher Olah - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* RNN Lecture from MIT 2020 Deep Learning course - https://www.youtube.com/watch?v=SEnXr6v2ifU&t=857s\n",
    "* Corresponding slides - http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L2.pdf\n",
    "* A deeper dive into challenges - https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html\n",
    "* Andrej Karpathy's post (mainly on character-level models) - http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "* Some other tutorial (maily captures the differences between vanilla RNN, LSTM and GRU) - https://www.youtube.com/watch?v=lycKqccytfU&feature=emb_logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Tradeoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have a training set consisting of a set of points $x_{1},\\dots ,x_{n}$ and real values $ y_{i}$ associated with each point $x_{i}$. We assume that there is a function with noise ${\\displaystyle y=f(x)+\\varepsilon }$, where the noise, $\\varepsilon$ , has zero mean and variance $\\sigma ^{2}$.\n",
    "<br>\n",
    "We want to find a function ${\\displaystyle {\\hat {f}}(x;D)}$, that approximates the true function $f(x)$ as well as possible, by means of some learning algorithm based on a training dataset (sample)  ${\\displaystyle D=\\{(x_{1},y_{1})\\dots ,(x_{n},y_{n})\\}}$. We make \"as well as possible\" precise by measuring the mean squared error between $y$ and ${\\displaystyle {\\hat {f}}(x;D)}$: we want ${\\displaystyle (y-{\\hat {f}}(x;D))^{2}}$ to be minimal, both for $ x_{1},\\dots ,x_{n}$ and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the $y_{i}$ contain noise  $\\varepsilon$ ; this means we must be prepared to accept an irreducible error in any function we come up with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "${\\displaystyle \\operatorname {Var} [X]=\\operatorname {E} [X^{2}]-{\\Big (}\\operatorname {E} [X]{\\Big )}^{2}.} $\n",
    "<br>\n",
    "Rearranging, we get:\n",
    "\n",
    "${\\displaystyle \\operatorname {E} [X^{2}]=\\operatorname {Var} [X]+{\\Big (}\\operatorname {E} [X]{\\Big )}^{2}.}$ \n",
    "<br>\n",
    "Since $f$ is deterministic, i.e. independent of $D$,\n",
    "${\\displaystyle \\operatorname {E} [f]=f.}$ \n",
    "\n",
    "Thus, given ${\\displaystyle y=f+\\varepsilon }$ and ${\\displaystyle \\operatorname {E} [\\varepsilon ]=0}$ , implies ${\\displaystyle \\operatorname {E} [y]=\\operatorname {E} [f+\\varepsilon ]=\\operatorname {E} [f]=f.}$\n",
    "\n",
    "Also, since ${\\displaystyle \\operatorname {Var} [\\varepsilon ]=\\sigma ^{2},}$\n",
    "\n",
    "${\\displaystyle \\operatorname {Var} [y]=\\operatorname {E} [(y-\\operatorname {E} [y])^{2}]=\\operatorname {E} [(y-f)^{2}]=\\operatorname {E} [(f+\\varepsilon -f)^{2}]=\\operatorname {E} [\\varepsilon ^{2}]=\\operatorname {Var} [\\varepsilon ]+{\\Big (}\\operatorname {E} [\\varepsilon ]{\\Big )}^{2}=\\sigma ^{2}}$\n",
    "\n",
    "Thus, since $\\varepsilon$  and ${\\hat {f}}$ are independent, we can write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/bias-var.svg?sanitize=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/bias-var-1.svg?sanitize=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "${\\displaystyle \\operatorname {Bias} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}=\\operatorname {E} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}-f(x)} $\n",
    "and\n",
    "\n",
    "${\\displaystyle \\operatorname {Var} _{D}{\\big [}{\\hat {f}}(x;D){\\big ]}=\\operatorname {E} _{D}[{\\hat {f}}(x;D)^{2}]-\\operatorname {E} _{D}[{\\hat {f}}(x;D)]^{2}.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irreducible error is the error that canâ€™t be reduced by creating good models. It is a measure of the amount of noise in our data. Here it is important to understand that no matter how good we make our model, our data will have certain amount of noise or irreducible error that can not be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias and variance using bulls-eye diagram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/bias_var_diag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/high_var_high_bias.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a good model, we need to find a good balance between bias and variance such that it minimizes the total error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/tradeoff.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES\n",
    "* https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
    "* https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\n",
    "* https://towardsdatascience.com/bias-and-variance-in-linear-models-e772546e0c30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menumend",
   "language": "python",
   "name": "menumend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

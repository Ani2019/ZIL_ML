{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Ensemble Methods?\n",
    "\n",
    "> **Ensemble methods are meta-algorithms that combine several machine learning techniques(weak-learners) into one predictive model in order to get better results.**\n",
    "\n",
    "The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember Bias/Variance Tradeoff ?\n",
    "\n",
    "What is our desire? $\\longrightarrow$ **low bias and low variance model**\n",
    "\n",
    "The difficulty is they vary in opposite directions.\n",
    "To obtain good results we want to have \n",
    "* enough degrees of freedom to determine the underlying patterns in data\n",
    "* not too much degrees of freedom to avoid high variance (be more robust)\n",
    "\n",
    "Eventually, this is the bias-variance tradeoff.\n",
    "\n",
    "![b_v](../Images/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, what kind of models are considered as weak-learners?\n",
    "\n",
    "Weak learners (or base models) are models that perform not so well individually, because they have\n",
    "* high bias (low df)\n",
    "* high variance (high df)\n",
    "\n",
    "A strong learner is an ensemble (compination) of weak learners which reduces bias and/or variance and thus performes better.\n",
    "\n",
    "\n",
    "### Homogeneous and Heterogeneous Ensembles\n",
    "\n",
    "* a single base learning algorithm is used\n",
    "* different type of base learning algorithms are used\n",
    "\n",
    "\n",
    "### Sequential and Parallel ensemble methods\n",
    "\n",
    "* sequential ensemble methods where the base learners are generated sequentially (e.g. AdaBoost). Motivation is to exploit the dependence between the base learners. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.\n",
    "* parallel ensemble methods where the base learners are generated in parallel (e.g. Random Forest). Motivation is to exploit independence between the base learners since the error can be reduced dramatically by averaging.\n",
    "\n",
    "\n",
    "### Coherence between the models and aggregation method\n",
    "\n",
    "High bias, low variance models should be aggregated with method that tries to reduce bias.\n",
    "High variance, low bias models should be aggregated with method that tries to reduce variance.\n",
    "\n",
    "![ens_overview](../Images/ensemble_overview.png)\n",
    "\n",
    "## Three Main Meta-Algorithms for Ensemble Learning\n",
    "\n",
    "* **Bagging** \n",
    "    * aims to decrease the variance\n",
    "    * considers homogeneous weak-learners\n",
    "    * learns in parallel\n",
    "    * combines in a deterministic averaging method\n",
    "* **Boosting** \n",
    "    * aims to decrease bias\n",
    "    * considers homogeneous weak-learners\n",
    "    * learns sequentially\n",
    "    * combines in deterministic method\n",
    "* **Stacking**\n",
    "    * both\n",
    "    * considers heterogeneous weak-learnes\n",
    "    * learns in parallel\n",
    "    * combines by training a meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "#### *(stands for “bootstrap aggregating”)*\n",
    "\n",
    "### What is Bootstrapping?\n",
    "\n",
    "> **In statistics, bootstrapping is any test or metric that relies on random sampling with replacement.**\n",
    "\n",
    "We generate samples of size B (called bootstrap samples) from an initial dataset of size N by randomly drawing with replacement B observations.\n",
    "\n",
    "![btstrp](../Images/bootstrap.png)\n",
    "\n",
    "Samples generated by bootstrapping method can be considered as representative and independent samples of the true data distribution (almost i.i.d. samples). \n",
    "To make this approximation valid the following hypothesis are considered:\n",
    "1. **representativity** - #(N) should be large enough to capture all the data complexity\n",
    "2. **independence** - #(N) >> #(B) so that samples are not too much correlated\n",
    "\n",
    "Bootstrapping is often used to evaluate variance or confidence interval of some statistical estimators.\n",
    "![est_var](../Images/est_var.png)\n",
    "\n",
    "### Bagging Step by Step\n",
    "Keeping in mind that the training dataset comes from some true unknown underlying distribution (**theoretical variance of the training dataset**), the idea of bagging becomes more intuitive.\n",
    "> We want to fit several independent models and “average” their predictions in order to obtain a model with a lower variance.\n",
    "\n",
    "1. **Create multiple bootstrap samples so that each new bootstrap sample will act as another (almost) independent dataset drawn from true distribution.**\n",
    "\n",
    "$$\n",
    "\\left\\{z_{1}^{1}, z_{2}^{1}, \\ldots, z_{B}^{1}\\right\\},\\left\\{z_{1}^{2}, z_{2}^{2}, \\ldots, z_{B}^{2}\\right\\}, \\ldots,\\left\\{z_{1}^{L}, z_{2}^{L}, \\ldots, z_{B}^{L}\\right\\}\n",
    "$$\n",
    "\n",
    "where $z_{b}^{l} \\equiv$  $b^{th}$ observation of the $l^{th}$ bootstrap sample\n",
    "\n",
    "2. **Fit a weak learner for each of these samples.**\n",
    "\n",
    "$w_{1}(.), w_{2}(.), \\ldots, w_{L}(.)$\n",
    "\n",
    "3. **Aggregate them such that we kind of “average” their outputs and, so, obtain an ensemble model with less variance that its components.**\n",
    "\n",
    "    * simple average for regression problem \n",
    "    $$\n",
    "    s_{L}(.)=\\frac{1}{L} \\sum_{l=1}^{L} w_{l}(.)\n",
    "    $$\n",
    "\n",
    "    * hard voting (majority vote) for classification problem\n",
    "    $$\n",
    "    s_{L}(.)=\\underset{k}{\\arg \\max }\\left[\\operatorname{card}\\left(l | w_{l}(.)=k\\right)\\right]\n",
    "    $$\n",
    "    \n",
    "    * soft voting\n",
    "    $$\n",
    "    S_{l}(\\cdot)=\\arg \\max \\left[\\frac{1}{L} \\sum_{i=1}^{L} P\\left(w_{l}(\\cdot)\\right) | w_{l}(\\cdot)=k\\right]\n",
    "    $$\n",
    "\n",
    "Due to our hypothesis, the bootstrap samples are approximatively i.i.d. $\\Longrightarrow$ learned base models are i.i.d. too $\\Longrightarrow$ averaging them doesn't change expected value, but reduces variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "> **The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a “forest”), this model uses two key concepts that gives it the name random:**\n",
    "\n",
    "1. Random sampling of training data points when building trees\n",
    "2. Random subsets of features considered when splitting nodes\n",
    "\n",
    "Trees that compose a forest can be chosen to be either **shallow** (few depths) or **deep** (lot of depths, if not fully grown). Shallow trees have less variance but higher bias and then will be better choice for sequential methods that we will described thereafter. Deep trees, on the other side, have low bias but high variance and, so, are relevant choices for bagging method that is mainly focused at reducing variance.\n",
    "\n",
    "![rf](../Images/rand_forest.png)\n",
    "**The random forest combines hundreds or thousands of decision trees, trains each one on a slightly different set of the observations, splitting nodes in each tree considering a limited number of the features. The final predictions of the random forest are made by averaging the predictions of each individual tree.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    ">Boosting consists in, iteratively, fitting a weak learner, aggregate it to the ensemble model and “update” the training dataset to better take into account the strengths and weakness of the current ensemble model when fitting the next base model.\n",
    "\n",
    "![boosting](../Images/boosting.png)\n",
    "\n",
    "### Boosting Algorithms\n",
    "\n",
    "### 1. Adaboost\n",
    "#### *(stands for “adaptive boosting\")*\n",
    "\n",
    "![adaboost](../Images/adaboost.png)\n",
    "![samme](../Images/samme.png)\n",
    "\n",
    "\n",
    "## For more details check these papers:\n",
    "\n",
    "* [A Short Introduction to Boosting (Freund,Schapire)](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf)\n",
    "* [Multi-class AdaBoost (Zhu, Rosset, Zou,Hastie)](https://web.stanford.edu/~hastie/Papers/samme.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literature\n",
    "\n",
    "* https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n",
    "* https://scikit-learn.org/stable/modules/ensemble.html#forest\n",
    "* https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76\n",
    "* https://en.wikipedia.org/wiki/Ensemble_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
